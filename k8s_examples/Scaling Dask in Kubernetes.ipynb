{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Dask in Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. Dask overview\n",
    "2. Kubernetes overview\n",
    "3. Overview of installing Dask\n",
    "    1. Manually deploying pods and configuring Dask Scheduler/Workers\n",
    "    2. Installation using Helm\n",
    "    3. Installation using K8S YAML files\n",
    "4. Overview  of integrating Dask and Kubernetes\n",
    "    1. Scaling up/down cluster using dask_kubernetes\n",
    "        1. Adaptive\n",
    "        2. Manual\n",
    "    2. Scaling up/down cluster using kubectl\n",
    "        1. Auto-Scale\n",
    "            1. Custom Metrics\n",
    "        2. Manual\n",
    "    3.\n",
    "5. TODOS\n",
    "    1. How do we create a dask_kubernetes.KubeCluster based on a cluster deployed via helm\n",
    "    2. How do we configure custom GPU metrics for KubeCluster.adapt()\n",
    "        1. How do we install custom metric collection for K8s\n",
    "        2. How do we specify those limits in the dask_kubernetes library?\n",
    "    3.What are best practices for making storage available across worker pods?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Overview\n",
    "\n",
    "Dask is a flexible library for parallel computing in Python. It is purpose-built to parallelize python data science applications from a single laptop all the way up  to a complex 100+ node cluster.\n",
    "\n",
    "It is composed of a Dask scheduler and a scaling number of Dask workers and the APIs are designed to be familiar for anyone who has used Pandas or Numpy in the past.\n",
    "\n",
    "By itself Dask accelerates many machine learning applications, but when paired with the additional acclerations of GPUs integrated through the RAPIDS modules it becomes a very powerful tool that no data scientist should be without. \n",
    "\n",
    "For more information about dask see [here](https://docs.dask.org/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kubernetes\n",
    "Kubernetes (or K8S) is an open source tool for managing container workloads and services. K8S is designed to scale, and can run on single node systems all the way up to entire clouds.\n",
    "\n",
    "K8S allows you to deploy docker containers to run tasks. These docker containers are deployed in pods, which can have resources limitations defined, execution commands set, and allows you to specify custom docker images.\n",
    "\n",
    "K8S allows dynamic resources addition/removal and can be run on-prem, in the cloud, or using hybrid models.\n",
    "\n",
    "For more information about Kubernetes see [here](https://kubernetes.io/docs/home/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask and Kubernetes Integration\n",
    "\n",
    "By combining the cluster-management and auto-scaling capabilities of Kubernetes with the parallel computing and distributed resource management capabilities of dask we can create a data science environment that dynamically determines resources needs, grows to meets those needs, and optimally executes all data processesing, training, and inferencing in our cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of installing Dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually deploying pods and configuring Dask Scheduler/Workers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation using Helm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: These must  be run on the K8S managment server, not through this pod.\n",
    "!helm install -n rapids --namespace rapids --values helm/rapids.yml stable/dask\n",
    "!kubectl create -f k8s/roles.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation using K8S YAML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of integrating Dask and Kubernetes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up/down cluster using dask_kubernetes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_kubernetes as dk\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client\n",
    "cluster.adapt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_kubernetes as dk\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dk.KubeCluster.from_yaml('/rapids/notebooks/worker-spec-dynamic.yaml')\n",
    "cluster.scale(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up/down cluster using kubectl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto-Scale\n",
    "##### Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create -f k8s/auto-scale.yaml # TODO: This file isn't in the Dockerfile yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl -n rapids scale deployment rapids-dask-worker --replicas=0 # TODO: This may not work in this Dockerfile yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl -n rapids scale deployment rapids-dask-worker --replicas=3 # TODO: This may not work in this Dockerfile yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS\n",
    "1. How do we create a dask_kubernetes.KubeCluster based on a cluster deployed via helm\n",
    "2. How do we configure custom GPU metrics for KubeCluster.adapt()\n",
    "3. How do we install custom metric collection for K8s\n",
    "4. How do we specify those limits in the dask_kubernetes library? 3.What are best practices for making storage available across worker pods?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
